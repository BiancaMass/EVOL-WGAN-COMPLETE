
#### Training parameters ####
RANDN = False
GAN_BATCH_SIZE = 25
N_EPOCHS = 40
N_LAYERS = 1

# These were kept fixed
# LR_G = 0.01  # learning rate for the generator
# # Betas, initial decay rate for the Adam optimizer
# # Check: if these values are appropriate
# B1 = 0    # Beta1, the exponential decay rate for the 1st moment estimates. Default would be 0.9
# B2 = 0.9  # Beta2, the exponential decay rate for the 2nd moment estimates. Default would be 0.999
# LAMBDA_GP = 10  # Coefficient for the gradient penalty

