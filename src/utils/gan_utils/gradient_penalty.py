"""
Script adapted from 'How to calculate the gradient penalty proposed in "Improved Training of
Wasserstein GANs"?', 30/07/2021. Aray Karjauv. Stack Overflow.
https://ai.stackexchange.com/questions/29912/how-to-calculate-the-gradient-penalty-proposed-in-improved-training-of-wasserst
License CC BY-SA 4.0 DEED: https://creativecommons.org/licenses/by-sa/4.0/
"""

import torch
import torch.autograd as autograd


def compute_gp(critic_net, real_images, fake_images):
    """
    Calculates the gradient penalty loss for Wasserstein GAN with gradient penalty (WGAN-GP).

    The gradient penalty encourages the gradients of the discriminator's scores wrt the
    interpolated samples to have a norm of 1. Steps to achieve this:
    1. Create epsilon, a vector of small randomly sampled values of shape [batch_size, 1, 1, 1]
    2. Randomly interpolate the real and fake samples using epsilon-weighted average.
    3. Compute the critic's scores for the interpolated samples.
    4. Compute the gradient of the scores w.r.t. the interpolated samples
    5. Flatten the gradients
    6. Calculate gradient penalty = mean squared difference of the L2 (Euclidean) norm of
    the gradients.

    Returns the computed gradient penalty loss.
    :param critic_net: torch.nn.Module. The critic network.
    :param real_images: torch.Tensor. The real samples from the data distribution. Shape: (batch_size, C, W, H)
    :param fake_images: torch.Tensor. The fake samples generated by the generator. Shape: (batch_size, C, W, H)

    :return: torch.Tensor: The computed gradient penalty loss.
    """
    device = real_images.device

    batch_size = real_images.size(0)
    # Sample Alpha: small noise drawn from a uniform distribution
    alpha = torch.rand(batch_size, 1, 1, 1)
    # Make epsilon match the shape of real_images
    alpha = alpha.expand_as(real_images).to(device)

    # Getting x_hat (interpolated image between real images and generated images)
    interpolated_images = alpha * real_images + ((1 - alpha) * fake_images)

    # get scores for interpolated images
    interp_scores = critic_net(interpolated_images)
    # grad_outputs = torch.ones_like(interp_scores)
    grad_outputs = torch.ones(interp_scores.shape).to(device)

    # Calculate gradients of probabilities with respect to examples
    gradients = autograd.grad(
        outputs=interp_scores,
        inputs=interpolated_images,
        grad_outputs=grad_outputs,
        create_graph=True,
        retain_graph=True,
    )[0]

    # Compute and return Gradient Norm:

    # Gradients have shape (batch_size, num_channels, img_width, img_height),
    # so flatten to easily take norm per example in batch:
    # Reshape gradients to a 2D tensor. Each row is a flattened gradient for one sample of the batch
    gradients = gradients.view(batch_size, -1)
    # Compute the L2 (Euclidean) norm across the second dimension (dim=1)
    # grad_norm = gradients.norm(2, 1)
    # grad_penalty = torch.mean((grad_norm - 1) ** 2)

    # Derivatives of the gradient close to 0 can cause problems because of
    # the square root, so manually calculate norm and add epsilon
    grad_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)
    grad_penalty = ((grad_norm - 1) ** 2).mean()
    return grad_penalty

